#+TITLE: Notes from Mastering API Architecture

* Preface

After reading this book you will understand:

- The fundamentals of REST APIs and how to best build, version, and test APIs
- The architectural patterns involved in building an API platform
- The differences in managing API traffic at ingress and within service-to-service communication, and how to apply patterns and technologies such as API gateways and service meshes
- Threat modeling and key security considerations for APIs, such as authentication, authorization, and encryption
- How to evolve existing systems toward APIs and different target deployments, such as the cloud

And you will be able to:

- Design, build, and test API-based systems
- Help to implement and drive an organization’s API program from an architectural perspective
- Deploy, release, and configure key components of an API platform
- Deploy gateways and service meshes based on case studies
- Identify vulnerabilities in API architecture and implement measured security mitigations
- Contribute to emerging API trends and the associated communities

* Introduction

The authors consider API to mean the following:
- an api represents an abstraction of the underlying implementation
- it is represented by a specification that introduces types
- it has defined semantics or behavior to effectively model the
  exchange of information
- it enables extension to customers or third parties for business
  integration

** Using Architecture Decision Records

There are 4 key sections in an ADR: status, context, decision, and
consequences

* Part I: Designing, Building, and Testing APIs

This section provides the foundational building blocks for API-driven
architectures.

In Chapter 1 you will learn about REST and Remote Procedure Call
(RPC)–based APIs. We will explore specifications and schemas,
recommended standards, strategies for versioning, and how to choose
the right API for your system.

In Chapter 2 you will learn about testing APIs and how different test
styles are best applied to API-driven architectures.

* Chapter 1. Design, Build, and Specify APIs

** Introduction to REST

REpresentational State Transfer (REST) is a set of architectural
constraitns, most commonly applied using HTTP as the underlying
transport protocol.

To be considered restful, our API must ensure that:
- A producer-to-consumer interaction is modeled where the producer
  models resources the consumer can interact with.
- Requests from producer to consumer are stateless, meaning that the
  producer doesn't cache details of the previous request.
- Requests are cachable, meaning the producer can provide hints to the
  consumer where this is appropriate. In HTTP, this is often provided
  in information contained in the header.
- A uniform interface is conveyed to the consumer.
- It is a layered system, abstracting away the complexity of systems
  sitting behind the REST interface.

** The Richardson Maturity Model

*Level 0 - HTTP/RPC*

Establishes that the API is built using HTTP and has the notion of a
single URI. Taking our preceding example of /attendees and not
applying a verb to specify intent, we would open up an endpoint for
exchange. Essentially this represents an RPC implementation over the
REST protocol.

*Level 1 - Resources*

Establishes the use of resources and starts to bring in the idea of
modeling resources in the context of the URI. In our example, if we
added GET /attendees/1 returning a specific attendee, it would start
to look like a level 1 API. Martin Fowler draws an analogy to the
classic object-oriented world of introducing identity.

*Level 2 - Verbs (Methods)*

Starts to introduce the correct modeling of multiple resource URIs
accessed by different request methods (also known as HTTP verbs) based
on the effect of the resources on the server. An API at level 2 can
make guarantees around GET methods not impacting server state and
presenting multiple operations on the same resource URI. In our
example adding DELETE /attendees/1, PUT /attendees/1 would start to
add the notion of a level 2–compliant API.

*Level 3 - Hypermedia Controls*

This is the epitome of REST design and involves navigable APIs by the
use of HATEOAS (Hypertext As The Engine Of Application State). In our
example, when we call GET /attendees/1, the response would contain the
actions that are possible on the object returned from the server. This
would include the option to be able to update the attendee or delete
the attendee and what the client is required to invoke in order to do
so. In practical terms level 3 is rarely used in modern RESTful HTTP
services, and although the navigation is a benefit in flexible UI
style systems, it doesn’t suit interservice API calls. Using HATEOAS
would be a chatty experience and is often short-circuited by having a
complete specification of possible interactions up front while
programming against the producer.

---

When designing API exchanges, the different levels of Richardson
maturity are important to consider. Moving towards level 2 will enable
you to project an understandable resource model to the consumer, with
approporiate actions against the model.

** Introduction to RPC APIs

A remote procedure call (RPC) involves calling a method in one process
but having it execute code in another process.

gRPC is a modern open source high performance RPC. The gRPC attendee
service starts and exposes a gRPC server on a specified port, allowing
methods to be invoked remotely.

A key difference between RPC and REST is state. REST is by definition
stateless, whereas RPC state depends on the implementation. RPC
integrations in certain situations can also build up state as part of
the exchange, which has the convenience of high performance at the
potential cost of reliability and routing complexities.

** A Brief Mention of GraphQL

- RPC offers access to a series of individual functions provided by a
  producer, but does not usually extend a model or abstraction to the
  consumer.
- REST extends a resource model for a single API provided by the producer
  - the consumer needs to query sequentially to build up state on the
    client side. This is wasteful if the consumer is only interested
    in a subset of the fields on the response.

GraphQL introduces a technology later over existing services,
datastores, and APIs that provides a query language to query across
multiple sources. It uses the GraphQL schema language to specify the
types in individual APIs and how they combine.

GraphQL excels when a consumer requires uniform API access over a wide
range of interconnected services.

** REST API Standards and Structure

Some considerations:
- pagination using nextLink
- filtering collections
- error handling

** Summary

In this chapter we have covered how to design, build, and specify APIs and the different circumstances under which you may choose REST or gRPC. It is important to remember that it is not REST versus gRPC, but rather given the situations, which is the most appropriate choice for modeling the exchange. The key takeaways are:

- The barrier to building REST- and RPC-based APIs is low in most technologies. Carefully considering the design and structure is an important architectural decision.

- When choosing between REST and RPC models, consider the Richardson
  Maturity Model and the degree of coupling between the producer and
  consumer.

- REST is a fairly loose standard. When building APIs, conforming to
  an agreed API standard ensures your APIs are consistent and have the
  expected behavior for your consumers. API standards can also help to
  short-circuit potential design decisions that could lead to an
  incompatible API.

- OpenAPI Specifications are a useful way of sharing API structure and
  automating many coding-related activities. You should actively
  select OpenAPI features and choose what tooling or generation
  features will be applied to projects.

- Versioning is an important topic that adds complexity for the
  producer but is necessary to ease API usage for the consumer. Not
  planning for versioning in APIs exposed to consumers is
  dangerous. Versioning should be an active decision in the product
  feature set and a mechanism to convey versioning to consumers should
  be part of the discussion.

- gRPC performs incredibly well in high-bandwidth exchanges and is an
  ideal option for east–west exchanges. Tooling for gRPC is powerful
  and provides another option when modeling exchanges.

- Modeling multiple specifications starts to become quite tricky,
  especially when generating from one type of specification to
  another. Versioning complicates matters further but is an important
  factor to avoid breaking changes. Teams should think carefully
  before combining RPC representations with RESTful API
  representations, as there are fundamental differences in terms of
  usage and control over the consumer code.

The challenge for an API architecture is to meet the requirements from
a consumer business perspective, to create a great developer
experience around APIs, and to avoid unexpected compatibility
issues.



BUILD: make an example api that hits level 2 [done]
BUILD: try out gRPC between 2 services [done]
BUILD: try out graphql [done]

* Chapter 2. Testing APIs

** Test Quadrant


Q1

Unit and component tests for technology. These should verify that
the service that has been created works, and this verification should
be performed using automated testing.

Q2

Tests with the business. These ensure what is being built is
serving a purpose. This is verified with automated testing and can
also include manual testing.

Q3

Testing for the business. This is about ensuring that functional
requirements are met and also includes exploratory testing. When
Figure 2-2 was originally created, this type of testing was
manual; now it is possible to perform automated testing in this
area as well.

Q4

Ensuring that what exists works from a technical standpoint. From
Q1 you know that what has been built works; however, when the
product is being used, is it performing as expected? Examples of
performing correctly from a technical standpoint could include
security enforcement, SLA integrity, and autoscaling.

** Test Pyramid

Unit Tests > integration tests > end to end tests

** Contract Testing

Contract testing has 2 components: a consumer and a producer. A
consumer requests data from an API, and a producer responds to the API
requests.


BUILD: Build an api test with a contract in python


 But the real value added of this integration strategy is the fact
 that it fosters conversations and collaboration between
 teams. Whenever you need to introduce a breaking change as a
 provider, you can immediately verify which are the consumers that
 will be affected by the change, and initiate a process that involves
 both teams with the final goal of keeping your platform up, running
 and healthy.


https://docs.pact.io/

A key benefit of contract tests is that once the producer agrees to
implement a contract, this decouples the dependency of building the
consumer and producer.

It is worth looking at tooling that writes contracts for you based on
an OpenAPI specification.


** ADR Guideline: Contract Testing

*Decision:*

When building an API should you use contract testing and, if so,
should you use consumer-driven contracts or producer contracts?

*Discussion Points:*

Determine whether you are ready to include contract testing as part of
your API testing.

- Do you want to add an extra layer of testing to your API that
  developers will be required to learn about?

If contracts have not been used before, then it requires time to
decide how you will use them.

- Should contracts be centralized or in a project?

- Do additional tools and training need to be provided to help people
  with contracts?

If deciding to use contracts, then which methodology should be
used—CDC or producer contracts?

- Do you know who will use this API?

- Will this API be used just within your organization?

- Does the API have consumers that are willing to engage with you to
  help drive your functionality?

*Recommendations:*

We recommend using contract testing when building an API. Even if
there is a developer learning curve and you are deciding how you are
going to set up your contracts for the first time, we believe it is
worth the effort. Defined interactions that are tested save so much
time when integrating services together.

If you are exposing your API to a large external audience, it is
important to use producer contracts. Again, having defined
interactions that help ensure that your API does not break backward
compatibility is crucial.

If you’re building an internal API, the ideal is to work toward CDC,
even if you have to start with producer contracts and evolve over to
CDC.

If contract testing is not feasible, then for a producer you need
alternatives to ensure that your API is conforming your agreed
interactions and provide a way that consumers can test. This means
that you have to be very careful with your tests that the responses
and requests match with what is expected, which can be tricky and
time-consuming.

** API Component Testing

Component testing can be used to validate that multiple units work
together and should be used to validate behaviour.


 for APIs you would be looking to validate cases such as:

 Is the correct status code returned when a request is made?

 Does the response contain the correct data?

 Is an incoming payload rejected if a null or empty parameter is
 passed in?

 When I send a request where the accepted content type is XML, will
 the data return the expected format?

 If a request is made by a user who does not have the correct
 entitlements, what will the response be?

 What will happen if an empty dataset is returned? Is this a 404 or
 is it an empty array?

 When creating a resource, does the location header point to the
 new asset created?

** Using Stub Servers: Why and How

If you are using contract tests, the generated stub servers can be
used to verify that the consumer can communicate with the producer.

A generated stub server from a contract is not always available and
other options are required, as in the case of testing with an external
API, such as the Microsoft Graph API, or within your organization when
contracts are not used. The simplest one is to hand roll a stub server
that mimics the requests and responses of the service you interact
with.

*Decision*
Should integration testing be added to API testing?

*Discussion Points*

If your API is integrating with any other service, what level of
integration test should you use?

- Do you feel confident that you can just mock responses and do not
  need to perform integration tests?

- For creating a stub server to test against, are you able to
  accurately craft the request and responses or should they be
  recorded?

- Will you be able to keep stub servers up-to-date and recognize if an
  interaction is incorrect?

If your stubs are incorrect or become out of date, this means it is
possible to have tests that pass against your stub server, but when
you deploy to production, your service fails to interact with the
other API as it has changed.

*Recommendations*

We do recommend using the generated stub servers from contract
tests. However, if this is not available, then having integration
testing using recordings of interactions is the next best
option. Having integration tests that can be run locally gives
confidence that an integration will work, especially when refactoring
an integration; it will help to ensure that any changes have not
broken anything.

** Test Containers

Basically, run a container with all the dependencies locally and test
there.

testcontainers is a library (with python integration) that
orchestrates containers with a testing library:

https://testcontainers.com/

The authors suggest using test containers to test integrations.

** End to End Testing

For end to end testing, it is ideal to have real versions of our
actual services running and interacting together.

** ADR Guideline: End to End testing

*Decision*

As part of your testing setup, should you use automated end-to-end
tests?

*Discussion Points:*

Determine how complex your setup is to enable end-to-end testing. Do
you have a good idea of end-to-end tests that you require and will
provide value? Are there any specific requirements or more advanced
end-to-end tests that you should add?

*Recommendations:*

We recommend that you do perform at a minimum end-to-end testing on
core user journeys. This is going to give feedback as early as
possible in your development cycle that a user could be impacted with
the changes that have been made. Ideally you can run these end-to-end
tests locally; however, if not, then it should be part of your build
pipeline.

End-to-end testing is valuable but must be balanced against the time
investment you need to get it running. If it is not possible to do
automated end-to-end testing, then you need to have a run book of
manual tests that you can use. This run book should be used against a
testing environment before a production release. This type of manual
testing will considerably slow down your production releases and
ability to deliver value to customers.

** Key Takeaways

- make unit testing a core of our API
- Contract testing can help us develop a consistent API and test with
  other APIs
- Perform service tests on our components and isolate the integrations
  to validate incoming and outgoing traffic
- Use e2e tests to replicate core user journeys to help validate that
  our apis integrate correctly
- use ADR guidelines as a way to work out if we should add different
  tests to our api

* Part II. API Traffic Management

This part deals with traffic entering the system (ingress) and traffic
originating within the system (service to service traffic.)

* Chapter 3. API Gateways: Ingress Traffic Management

** Is an API Gateway the only solution?

No, but it is the most commonly used solution, particularly in an
enterprise context. As the number of consumers and providers
increases, it is often the most scalable, maintaianable, and secure
option.

|--------------------+---------------+---------------+-------------|
| Feature            | Reverse proxy | Load balancer | API gateway |
|--------------------+---------------+---------------+-------------|
| Single Backend     | *             | *             | *           |
| TLS/SSL            | *             | *             | *           |
| Multiple Backends  |               | *             | *           |
| Service Discovery  |               | *             | *           |
| API Composition    |               |               | *           |
| Authorization      |               |               | *           |
| Retry Logic        |               |               | *           |
| Rate Limiting      |               |               | *           |
| Logging and Tracing|               |               | *           |
| Circuit Breaking   |               |               | *           |
|--------------------+---------------+---------------+-------------|

** ADR: Proxy, Load Balancer, or API Gateway

*Decision*

Should you use a proxy, load balancer, or API gateways for routing
ingress traffic?

*Discussion Points*

Do you want simple routing, for example, from a single endpoint to a single backend service?

Do you have cross-functional requirements that will require more advanced features, such as authentication, authorization, or rate limiting?

Do you require API management functionality, such as API keys/tokens or monetization/chargeback?

Do you already have a solution in place, or is there an organization-wide mandate that all traffic must be routed through certain components at the edge of your network?

*Recommendations*

Always use the simplest solution for your requirements, with an eye to the immediate future and known requirements.

If you have advanced cross-functional requirements, an API gateway is typically the best choice.

If your organization is an enterprise, an API gateway that supports API Management (APIM) features is recommended.

Always perform due diligence within your organization for existing
mandates, solutions, and components.

** What is an API Gateway?

An API gateway is a management tool that sits at the edge of a system
between a consumer and a collection of backend services and acts as a
single point of entry for a defined group of APIs.

An API gateway is implemented with 2 high-level fundamental
components: a control plane and a data plane.The control plane is
where operators interact with the gateway and define routes, policies,
and required telemetry. The data plane is where all of the work
specified in the control plane occurs, the network packets are routed,
the policies enforced, and telemetry emitted.

At a network level an API gateway typically acts as a reverse proxy to
accept all of the api requests from a consumer, calls and aggregates
the various application-level backend services required to fulfill
them, and returns the appropriate result.

It also provides requirements such as user authentication, request
rate limiting, timeouts/retries, metrics, logs, and trace data.


#+DOWNLOADED: screenshot @ 2024-04-10 08:19:30
[[file:Chapter_3._API_Gateways:_Ingress_Traffic_Management/2024-04-10_08-19-30_screenshot.png]]

** Why use an API gateway?

This section of the chapter will provide you with an overview of the key problems that an API gateway can address, such as:

- Reducing coupling by using an adapter/facade between frontends and
  backends
- Simplifying consumption by aggregating/translating backend services
- Protecting APIs from overuse and abuse with threat detection and
  mitigation
- Understanding how APIs are being consumed (observability)
- Managing APIs as products with API lifecycle management
- Monetizing APIs by using account management, billing, and pay

** API Gateway as a single point of failure

In a standard web-based system, the first obvious points of failure
are (in order):

- DNS
- global and regional layer load balancers
- security edge components (like firewall or WAF)
- API gateway

** ADR Guideline: Selecting an API Gateway

*Decision*

How should we approach selecting an API gateway for our organization?

*Discussion Points*

Have we identified and prioritized all of our requirements associated
with selecting an API gateway?

Have we identified current technology solutions that have been
deployed in this space within the organization?

Do we know all of our team and organizational constraints?

Have we explored our future roadmap in relation to this decision?

Have we honestly calculated the “build versus buy” costs?

Have we explored the current technology landscape and are we aware of
all of the available solutions?

Have we consulted and informed all involved stakeholders in our
analysis and decision making?

*Recommendations*

Focus particularly on your requirement to reduce API/system coupling,
simplify consumption, protect APIs from overuse and abuse, understand
how APIs are being consumed, manage APIs as products, and monetize
APIs.

Key questions to ask include: is there an existing API gateway in use?
Has a collection of technologies been assembled to provide similar
functionality (e.g., hardware load balancer combined with a monolithic
app that performs authentication and application-level routing)? How
many components currently make up your edge stack (e.g., WAF, LB, edge
cache, etc.)?

Focus on technology skill levels within your team, availability of
people to work on an API gateway project, and available resources and
budget, etc.

It is important to identify all planning changes, new features, and
current goals that could impact traffic management and the other
functionality that an API gateway provides.

Calculate the total cost of ownership (TCO) of all of the current API
gateway-like implementations and potential future solutions.

Consult with well-known analysts, trend reports, and product reviews
in order to understand all of the current solutions available.

Selecting and deploying an API gateway will impact many teams and
individuals. Be sure to consult with the developers, QA, the
architecture review board, the platform team, InfoSec, etc.

** Summary

An API gateway is a tool that sits at the edge of the system, handles
routing to various backend services, and provides middleware concerns
like security, logging, adapters for various implementations, etc.

They help with managing north-south ingress traffic.

* Chapter 4. Service Mesh: Service-to-Service Traffic Management

Now we learn about managing traffic for internal APIs, i.e.,
service-to-service communication.

Service mesh implementations provide functionality for routing,
observing, and securing traffic for service-to-service communication.

Both a library (like an SDK or driver) and service mesh-based
solutions can satisfy s2s needs.

** ADR: Service Mesh

*Decision*
Should you use a service mesh or a library for routing service traffic?

*Discussion Points*

Do you use a single programming language within your organization?

Do you only require simple service-to-service routing for REST or
RPC-like communication?

Do you have cross-functional requirements that will require more
advanced features, such as authentication, authorization, or rate
limiting?

Do you already have a solution in place, or is there an
organization-wide mandate that all traffic must be routed through
certain components within your network?

*Recommendations*

If your organization mandates the use of a single programming language
or framework, you can typically take advantage of the
language-specific libraries or mechanisms for service-to-service
communication.

Always use the simplest solution for your requirements, with an eye to
the immediate future and known requirements.

If you have advanced cross-functional requirements, particularly
across services that use different programming languages or technology
stacks, a service mesh may be the best choice.

Always perform due diligence within your organization for existing
mandates, solutions, and components.

** What is a Service Mesh?

It is a pattern for managing all service to service communication
within a distributed software system. The originator of the
communication is typically a somewhat known internal service, rather
than a user's device or a system running external to our applications.

It is implemented with 2 high-level fundamental components: a control
plane and a data plane, which are deployed separately. The control
plane is where operators interact with the service mesh and define
routes, policies, and required telemetry. The data plane is the
location where all of the work specified in the control plane occurs
and where the network packets are routed, the policies enforced, and
telemetry emitted.

** What functionality does a service mesh provide?

At a network level, a service mesh proxy acts as a full proxy,
accepting all inbound traffic from other services and also initiating
all outbound requests to other services, including all API calls and
other requests.

** Why use a service mesh?

• Enable fine-grained control of service routing, reliability, and traffic management
• Improve observability of interservice calls
• Enforce security, including transport encryption, authentication,
• and authorization
• Support cross-functional communication requirements across a variety
• of languages
• Separate ingress and service-to-service traffic management

** Sidecarless: Operating System Kernel (eBPF) implementations

An alternative service mesh implementation is based on pushing the
required networking abstractions back into the operating system
itself. eBPF allows custom programs to run sandboxed within the
kernel. eBPF programs are run in response to OS-level events,
including things like entry to or exit from any function in the kernel
or user space, or "trace points" and "probe points", and, importantly
for service mesh, the arrival of network packets.

The big 3 implementations of service meshes:
- library based (and proxyless, e.g. grpc)
- sidecars, proxy based
- OS/kernel based

** Summary

- Fundamentally, “service mesh” is a pattern for managing all
  service-to-service communication within a distributed software
  system.

- At a network level, a service mesh proxy acts as a full proxy,
  accepting all inbound traffic from other services and also
  initiating all outbound requests to other services.

- A service mesh is deployed within an internal network or
  cluster. Large systems or networks are typically managed by
  deploying several instances of a service mesh, often with each
  single mesh spanning a network segment or business domain

- A service mesh may expose endpoints within a network demilitarized
  zone (DMZ), or to external systems, or additional networks or
  clusters, but this is frequently implemented by using an “ingress,”
  “terminating,” or “transit” gateway.

- There are many API-related cross-cutting concerns that you might
  have for each or all of your internal services, including: product
  lifecycle management (incrementally releasing new versions of a
  service), reliability, multilanguage communication support,
  observability, security, maintainability, and extensibility. A
  service mesh can help with all of these.

- A service mesh can be implemented using language-specific libraries,
  sidecar proxies, proxyless communication frameworks (gRPC), or
  kernel-based technologies like eBPF.

- The most vulnerable component of a service mesh is typically the
  control plane. This must be secured, monitored, and run as a highly
  available service.

- Service mesh usage antipatterns include: service mesh as ESB,
  service mesh as gateway, and using too many networking layers.

- Choosing to implement a service mesh, and selecting the technology
  to do so, are Type 1 decisions. Research, requirements analysis, and
  appropriate design must be conducted.

- If you have decided to adopt the service mesh pattern we believe
  that it is typically best to adopt and standardize on an open source
  implementation or commercial solution rather than build your own

* Chapter 5: Deploying and Releasing APIs

** Separating Deployment and Release

#+begin_quote
Implementing Continuous Delivery continues to be a challenge for many
organiza‐ tions, and it remains important to highlight useful
techniques such as decoupling deployment from release. We recommend
strictly using the term Deployment when referring to the act of
deploying a change to application components or infrastructure.  The
term Release should be used when a feature change is released to end
users, with a business impact. Using techniques such as feature
toggles and dark launches, we can deploy changes to production systems
more frequently without releasing features.  More-frequent deployments
reduce the risk associated with change, while *business stakeholders
retain control over when features are released to end users.*

- Thoughtworks Technology Radar 2016
#+end_quote

*** Case Study: Feature Flagging

Feature flags are typically hosted in a configuration store outside of
the running app and allow code to be deployed with the feature
off. Once the team is ready to enable the feature, they can toggle the
feature on, which causes the application to execute a different branch
of code.

A simple example is setting an env var to false, then to true and
having an if statement that checks against the env var before running
a piece of code.

Knight capital went insolvent in minutes because they reused a feature
flag.

** Release Strategies

This section discusses mechanisms for controlling the progressive
release of features. It focuses on reducing risk in production.

*** Canary Releases

Introduces a new version of the software and flows a small percentage
of the traffic to the canary. The percentage of traffic exposed to the
canary is highly controlled. The trade-off is that you must have good
monitoring in place to quickly identify an issue and roll back if
necessary. There is also the added advantage that only a single new
instance is spun-up -- in strategies like blue-green, a complete
second stack of services is needed.

*** Traffic Mirroring

This copies / duplicates traffic and sends it to an additional
location or series of locations. Usually the results of the duplicated
requests are not returned to the calling service or end user. Instead,
they are evaluated out of band for correctness, such as comparing the
results generated by a refactored and existing service, or a selection
of operational properties are observed as a new service version
handles the request, such as response latency or CPU required.

*** Blue-Green

Usually deployed at the point in the architecture that uses a router,
gateway, or load balancer, behind which sits a complete blue
environment and a green environment.

The blue environment represents the current live environment, and the
green environment represents the next version of the stack.

The green env is checked prior to switching to live traffic, and at go
live the traffic is switched from blue to green. The blue env is now
"off", but if a problem is spotted it is a quick rollback.


** Monitoring for Success and Identifying Failure

Observability is best described by 3 pillars, an operational minimum
required to reason about distributed architecture:

- Metrics are a measurement captured at regular intervals that
  represent an important element to the overall platform health.
- Logs
- Traces

SRE 4 Golden Signals: latency, traffic, errors, and saturation

In the example system, here are examples of metrics to capture:

- The number of requests per minute for attendees.
- The service-level objective (SLO) for attendees is average latency
- for responses. If the latency starts to significantly deviate, it
  could be the early signs of an issue.
- Number of 401s from the CFP system could indicate a vendor
- compromise or a stolen token.
- Measure of availability and uptime of the Attendee service.
- Memory and CPU usage of the applications.
- The total number of attendees in the system

** Application Decisions for Effective Software Releases

- Response caching
  - caches can hide errors during deployments, since the endpoint may be returning cached
    data
- Application-level header propagation
  - Any api services that terminate an api request and create a
    request to another service need to copy headers across from the
    terminated request to the new request.
- Logging to assist debugging
  - It is useful to think of logs in 2 different types
    - *journal* allows the capturing of important transactions/events
      within the system and is used sparingly
    - *diagnostics* are more concerned with failures

** Summary

- A valuable starting point is to understand the importance of
  separating deployment and release. In existing applications, feature
  flagging is one approach to configuring and enabling new features at
  a code level.
- Traffic management provides a new opportunity to use the routing of
  traffic to model releases.
- Major, minor, and patch releases help to separate the style of
- release options. Applications that have a tightly coupled API may
  use a different strategy.
- You have reviewed the release strategies and the situations in which
  they apply, and you saw how tools like Argo can help to facilitate
  rollouts effectively.
- Monitoring and metrics are an important measure of success in an API
  platform. You have reviewed why some metrics can be gotchas and
  could suggest a problem where there isn’t one. You have learned a
  primer to observability and why applying these technologies is
  critical to successfully operating an API platform.
- Finally, you explored application decisions to support effective
  rollouts and what platform owners may wish to consider when aiming
  for consistency across the plant.

* Operational Security: Threat Modeling for APIs

1. *Identify your objectives* Create a list of the business and security
   objectives. Keep them simple (e.g., avoid unauthorized access).

2. *Gather the right information* Generate a high-level design of the
   system and ensure you have the right information. To be able to
   understand how your systems work and work together, this will
   include having the right people involved in the conversation.

3. *Decompose the system* Break down your high-level design so that you
   can start to model the threats. This may require multiple models
   and diagrams.

4. *Identify threats* Systematically look for threats to your systems.

5. *Evaluate the risk of the threats* Prioritize threats to focus on the
   most likely ones, then identify mitigations to these likely
   threats.

6. *Validate* Ask yourself and your team if the changes in place have
   been successful. Should you perform another review?

** STRIDE Methodology

*** STRIDE Methodology
**** Spoofing
   - Impersonation of something or someone else to gain unauthorized access.
**** Tampering
   - Unauthorized modification of data.
**** Repudiation
   - Actions that cannot be traced back to the perpetrator.
**** Information Disclosure
   - Sensitive data is exposed to unauthorized parties.
**** Denial of Service (DoS)
   - Making a resource unavailable to its intended users.
**** Elevation of Privilege
   - Attacker gains higher-level permissions than originally assigned.

*** Spoofing

Ensure that a person is not able to masquerade as another person or
program. To do this, we must authenticate any requests that are made
and ensure that they are legitimate.

*** Tampering

Users or clients should not be able to modify the system, app, or data
in an unintended manner. There are 2 primary ways that tampering
occurs: through payload injection and mass assignment

**** Payload Injection

The attacker attempts to inject a malicious payload into the request
made to an API or app. We can aim to prevent injection attacks early
in the request handling chain by using the API gateway to validate
that the request made conforms to a defined contract or schema. This
can be helped with strict validation on the API input data.

**** Mass Assignment

Modifiable properties that are bound to database entities are
vulnerable to being inappropriately changed. Mass assignment is
typical where client input data is bound to internal objects without
thought of the repercussions, which is often a consequence when
exposing a database API as a web-based API.

**** Repudiation

A repudiation attack happens when an application or system does not
adopt controls to properly track and log user's actions, which permits
manipulation or forging the identification of new actions. Ensure that
there is sufficient logging and monitoring.

**** Information Disclosure

Do not expose information that should be used internally or kept
secret.

***** Excessive Data Exposure

Do not expose any data that is sensitive or not needed.

***** Improper Assets Management

This typically occurs as our systems evolve, and the org loses track
of which APIs and which versions are exposed, or which APIs were
designed for internal consumption only.

**** Denial of Service

Rate limiting and load shedding

Rate limiting limits the number of requests that can be made to our
API over a period of time. Load shedding means rejecting requests
based on the overall state of the system.

**** Elevation of Privilege

This occurs when a user or app finds a way to perform a task that is
outside the scope of what should be allowed given the current security
context.

**** Extras

***** Security Misconfiguration
Things like always having TLS (transport layer security) and ip
allowlisting.

***** TLS termination
TLS ensures that the traffic we receive has not been intercepted and modified.

***** CORS (Cross-Origin Request Sharing)
CORS is an http-header-based mechanism that allows a server to
indicate any origins (domain, scheme, or port) other than it's own
from which a browser should permit loading resources.

***** Security Directive Hardening

A request to an API endpoint can contain an arbitrary payload. This
ensures that all genuine requests conform to an expected contract.

** Evaluate Threat Risks

To evaluate threats, we can employ a qualitative risk calculation
known as DREAD. Like STRIDE, DREAD was developed at M$.

*** DREAD Methodology
**** Damage
   - Potential damage that could be caused if the threat were to be successfully exploited.
**** Reproducibility
   - How easy it is to reproduce the attack.
**** Exploitability
   - Ease of launching an attack using this vulnerability.
**** Affected Users
   - How many users would be impacted if the attack were successful.
**** Discoverability
   - How easy it is for an attacker to discover the vulnerability.

*** Alternatives to DREAD
**** MITRE ATT&CK Framework
   - A comprehensive knowledge base of adversary tactics and techniques based on real-world observations.
**** PASTA (Process for Attack Simulation and Threat Analysis)
   - A seven-step, risk-centric methodology integrating security risk management with software development.
**** Trike
   - Focuses on defining acceptable levels of risk from the perspective of stakeholders.
**** VAST (Visual, Agile, and Simple Threat modeling)
   - Designed to be scalable and usable in agile development processes, integrating security into DevOps.

** Summary
*** Financial penalties and reputational damage
  - Strong consequences for not securing APIs.
*** Beginning Threat Modeling for API Systems
  - Start with creating a data flow diagram (DFD).
  - Use automated tooling for rapid analysis and threat identification.
*** Conducting Threat Modeling Without Being a Security Expert
  - Important skill: "thinking like an attacker."
*** Threat Modeling Process
  - Identify your objectives.
  - Gather the right information.
  - Decompose the system.
  - Identify threats.
  - Evaluate the risk of those threats.
  - Validate the results and actions.
*** Resources for Understanding Threats
  - OWASP API Security Top 10.
*** STRIDE Methodology Focus
  - Addressing the threats of:
    + Spoofing
    + Tampering
    + Repudiation
    + Information disclosure
    + Denial of service
    + Elevation of privilege.
*** DREAD Methodology Application
  - Calculate a qualitative risk metric to prioritize threats.
*** API Gateway in Distributed Systems
  - Provides high-level risk mitigation.
  - Consider individual service implementations and interservice communication in distributed systems.


* Chapter 7: API Authentication and Authorization
** End-User Authentication with Tokens

In token-based auth, the user enters their credentials, which is
exchanged for a token. The token is sent in the REST request as part
of the auth bearer header. Tokens are sensitive and must be sent over
HTTPS. The token should have a limited lifetime (e.g. an hour), and
after a token expires the user would need to obtain a new token.

Tokens have the advantage that long-lived credentials, like passwords,
don't need to cross the network for every network request to access
resources.

** System-to-System Authentication

You could use an API key and send that in the request header. The API
key should be non-guessable.

** OAuth2

OAuth2 allows a user to consent that a third-party app can access
their data on their behalf.

Think github access to different sites.

** JSON Web Tokens (JWT)

A JSON Web Token consists of claims and these claims have assocaited
values. They are structured and encoded using standards to ensure the
token is unmodifiable and additionally can be encrypted.

** OIDC (OpenID Connect)

OAuth2 provides a mechanism for the client to access APIs using authentication
and authorization. A common requirement is for the client to know the
identity of the resource owner, but OAuth2 grants do not provide a way
to obtain the identity of the end user.

OIDC provides an identity layer. This layer builds on top of OAuth2,
turning it into an OpenID provider as well. This way the client can
request information about a user by using a special scope called
openid.

** SAML 2.0

In enterprise environments it is common to use SAML 2.0. SAML
(Security Assertion Markup Language) is an open standard that
transfers assertions.

** Summary
- OAuth2 is the de facto standard for securing APIs and often
  leverages JWT as part of the bearer header. JWT tokens are often
  encoded and signed to ensure they are tamper free.
- Different OAuth2 grants support different scenarios.
- Refresh tokens help smooth out the end user experience of needing to
  keep asking the user to enter a username and password.
- OAuth2 scopes help to provide course-grained authorization and allow
  the end user to configrue the access of a client.
- OIDC is used when the client requires information about the end
  user.

* Chapter 8. Redesigning Applications to API-Driven Architectures
